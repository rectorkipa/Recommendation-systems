{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание № 7, Кривоногов Н.В."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import DoubleType\n",
    "import pyspark.sql.functions as sf\n",
    "\n",
    "# Для работы с матрицами\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Матричная факторизация\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "from implicit.nearest_neighbours import bm25_weight, tfidf_weight\n",
    "\n",
    "# Модель второго уровня\n",
    "# from lightgbm import LGBMClassifier\n",
    "\n",
    "import os, sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join(os.pardir))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Написанные нами функции\n",
    "from metrics import precision_at_k, recall_at_k\n",
    "from utils import prefilter_items\n",
    "# from recommenders import MainRecommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>basket_id</th>\n",
       "      <th>day</th>\n",
       "      <th>item_id</th>\n",
       "      <th>quantity</th>\n",
       "      <th>sales_value</th>\n",
       "      <th>store_id</th>\n",
       "      <th>retail_disc</th>\n",
       "      <th>trans_time</th>\n",
       "      <th>week_no</th>\n",
       "      <th>coupon_disc</th>\n",
       "      <th>coupon_match_disc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2375</td>\n",
       "      <td>26984851472</td>\n",
       "      <td>1</td>\n",
       "      <td>1004906</td>\n",
       "      <td>1</td>\n",
       "      <td>1.39</td>\n",
       "      <td>364</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>1631</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2375</td>\n",
       "      <td>26984851472</td>\n",
       "      <td>1</td>\n",
       "      <td>1033142</td>\n",
       "      <td>1</td>\n",
       "      <td>0.82</td>\n",
       "      <td>364</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1631</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id    basket_id  day  item_id  quantity  sales_value  store_id  \\\n",
       "0     2375  26984851472    1  1004906         1         1.39       364   \n",
       "1     2375  26984851472    1  1033142         1         0.82       364   \n",
       "\n",
       "   retail_disc  trans_time  week_no  coupon_disc  coupon_match_disc  \n",
       "0         -0.6        1631        1          0.0                0.0  \n",
       "1          0.0        1631        1          0.0                0.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('retail_train.csv')\n",
    "item_features = pd.read_csv('product.csv')\n",
    "user_features = pd.read_csv('hh_demographic.csv')\n",
    "\n",
    "# column processing\n",
    "item_features.columns = [col.lower() for col in item_features.columns]\n",
    "user_features.columns = [col.lower() for col in user_features.columns]\n",
    "\n",
    "item_features.rename(columns={'product_id': 'item_id'}, inplace=True)\n",
    "user_features.rename(columns={'household_key': 'user_id'}, inplace=True)\n",
    "\n",
    "# train test split\n",
    "test_size_weeks = 3\n",
    "\n",
    "data_train = data[data['week_no'] < data['week_no'].max() - test_size_weeks]\n",
    "data_test = data[data['week_no'] >= data['week_no'].max() - test_size_weeks]\n",
    "\n",
    "data_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreased # items from 86865 to 86865\n"
     ]
    }
   ],
   "source": [
    "n_items_before = data_train['item_id'].nunique()\n",
    "\n",
    "# data_train = prefilter_items(data_train, item_features)\n",
    "\n",
    "n_items_after = data_train['item_id'].nunique()\n",
    "print('Decreased # items from {} to {}'.format(n_items_before, n_items_after))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "popularity = data_train.groupby('item_id')['quantity'].sum().reset_index()\n",
    "popularity.rename(columns={'quantity': 'n_sold'}, inplace=True)\n",
    "\n",
    "top_5000 = popularity.sort_values('n_sold', ascending=False).head(5000).item_id.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.loc[~data_train['item_id'].isin(top_5000), 'item_id'] = 999999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>item_id</th>\n",
       "      <th>202291</th>\n",
       "      <th>397896</th>\n",
       "      <th>420647</th>\n",
       "      <th>480014</th>\n",
       "      <th>545926</th>\n",
       "      <th>707683</th>\n",
       "      <th>731106</th>\n",
       "      <th>818980</th>\n",
       "      <th>819063</th>\n",
       "      <th>819227</th>\n",
       "      <th>...</th>\n",
       "      <th>15778533</th>\n",
       "      <th>15831255</th>\n",
       "      <th>15926712</th>\n",
       "      <th>15926775</th>\n",
       "      <th>15926844</th>\n",
       "      <th>15926886</th>\n",
       "      <th>15927403</th>\n",
       "      <th>15927661</th>\n",
       "      <th>15927850</th>\n",
       "      <th>16809471</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 5001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "item_id  202291    397896    420647    480014    545926    707683    731106    \\\n",
       "user_id                                                                         \n",
       "1             0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "2             0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "\n",
       "item_id  818980    819063    819227    ...  15778533  15831255  15926712  \\\n",
       "user_id                                ...                                 \n",
       "1             0.0       0.0       0.0  ...       0.0       0.0       0.0   \n",
       "2             0.0       0.0       0.0  ...       0.0       0.0       0.0   \n",
       "\n",
       "item_id  15926775  15926844  15926886  15927403  15927661  15927850  16809471  \n",
       "user_id                                                                        \n",
       "1             0.0       1.0       0.0       0.0       0.0       0.0       0.0  \n",
       "2             0.0       0.0       0.0       0.0       0.0       0.0       0.0  \n",
       "\n",
       "[2 rows x 5001 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_item_matrix = pd.pivot_table(data_train, \n",
    "                                  index='user_id', columns='item_id', \n",
    "                                  values='quantity', # Можно пробовать другие варианты\n",
    "                                  aggfunc='count', \n",
    "                                  fill_value=0\n",
    "                                 )\n",
    "\n",
    "user_item_matrix = user_item_matrix.astype(float) # необходимый тип матрицы для implicit\n",
    "\n",
    "# переведем в формат sparse matrix\n",
    "sparse_user_item = csr_matrix(user_item_matrix).tocsr()\n",
    "\n",
    "user_item_matrix.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = data_test[data_test['item_id'].isin(data_train['item_id'].unique())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[821867, 834484, 856942, 865456, 914190, 95804...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>[851057, 872021, 878302, 879948, 909638, 91320...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                             actual\n",
       "0        1  [821867, 834484, 856942, 865456, 914190, 95804...\n",
       "1        3  [851057, 872021, 878302, 879948, 909638, 91320..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = data_test.groupby('user_id')['item_id'].unique().reset_index()\n",
    "result.columns=['user_id', 'actual']\n",
    "result.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "userids = user_item_matrix.index.values\n",
    "itemids = user_item_matrix.columns.values\n",
    "\n",
    "matrix_userids = np.arange(len(userids))\n",
    "matrix_itemids = np.arange(len(itemids))\n",
    "\n",
    "id_to_itemid = dict(zip(matrix_itemids, itemids))\n",
    "id_to_userid = dict(zip(matrix_userids, userids))\n",
    "\n",
    "itemid_to_id = dict(zip(itemids, matrix_itemids))\n",
    "userid_to_id = dict(zip(userids, matrix_userids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = (\n",
    "        SparkSession.builder.config('spark.driver.memory', '1g')\n",
    "        .config('spark.sql.shuffle.partitions', '100')\n",
    "        .config('spark.driver.bindAddress', '127.0.0.1')\n",
    "        .config('spark.driver.host', 'localhost')\n",
    "        .master('local[1]')\n",
    "        .enableHiveSupport()\n",
    "        .getOrCreate()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://localhost:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[1]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2508d96d790>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data_train['item_idx'] = data_train['item_id'].map(lambda x:itemid_to_id[x])\n",
    "# data_train['user_idx'] = data_train['user_id'].map(lambda x:userid_to_id[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "spark_data_train = session.createDataFrame(data_train[['user_id', 'item_id', 'quantity']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_data_train = spark_data_train.withColumnRenamed('quantity', 'relevance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark_data_train.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Py4JJavaError                             Traceback (most recent call last)\n",
    "# ~\\AppData\\Local\\Temp\\ipykernel_1552\\3177425222.py in <module>\n",
    "# ----> 1 spark_data_train.show(10)\n",
    "\n",
    "# C:\\Conda\\lib\\site-packages\\pyspark\\sql\\dataframe.py in show(self, n, truncate, vertical)\n",
    "#     604 \n",
    "#     605         if isinstance(truncate, bool) and truncate:\n",
    "# --> 606             print(self._jdf.showString(n, 20, vertical))\n",
    "#     607         else:\n",
    "#     608             try:\n",
    "\n",
    "# C:\\Conda\\lib\\site-packages\\py4j\\java_gateway.py in __call__(self, *args)\n",
    "#    1319 \n",
    "#    1320         answer = self.gateway_client.send_command(command)\n",
    "# -> 1321         return_value = get_return_value(\n",
    "#    1322             answer, self.gateway_client, self.target_id, self.name)\n",
    "#    1323 \n",
    "\n",
    "# C:\\Conda\\lib\\site-packages\\pyspark\\sql\\utils.py in deco(*a, **kw)\n",
    "#     188     def deco(*a: Any, **kw: Any) -> Any:\n",
    "#     189         try:\n",
    "# --> 190             return f(*a, **kw)\n",
    "#     191         except Py4JJavaError as e:\n",
    "#     192             converted = convert_exception(e.java_exception)\n",
    "\n",
    "# C:\\Conda\\lib\\site-packages\\py4j\\protocol.py in get_return_value(answer, gateway_client, target_id, name)\n",
    "#     324             value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n",
    "#     325             if answer[1] == REFERENCE_TYPE:\n",
    "# --> 326                 raise Py4JJavaError(\n",
    "#     327                     \"An error occurred while calling {0}{1}{2}.\\n\".\n",
    "#     328                     format(target_id, \".\", name), value)\n",
    "\n",
    "# Py4JJavaError: An error occurred while calling o58.showString.\n",
    "# : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (noutnik executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\n",
    "# \tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\n",
    "# \tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\n",
    "# \tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n",
    "# \tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:157)\n",
    "# \tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
    "# \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
    "# \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
    "# \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
    "# \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
    "# \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
    "# \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
    "# \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
    "# \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
    "# \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
    "# \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
    "# \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
    "# \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
    "# \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
    "# \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
    "# \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
    "# \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
    "# \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
    "# \tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
    "# \tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
    "# \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
    "# \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
    "# \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
    "# \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
    "# \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
    "# \tat java.base/java.lang.Thread.run(Thread.java:1623)\n",
    "# Caused by: java.net.SocketTimeoutException: Accept timed out\n",
    "# \tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\n",
    "# \tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\n",
    "# \tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\n",
    "# \tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\n",
    "# \tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\n",
    "# \tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\n",
    "# \tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\n",
    "# \tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\n",
    "# \t... 29 more\n",
    "\n",
    "# Driver stacktrace:\n",
    "# \tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n",
    "# \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n",
    "# \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n",
    "# \tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
    "# \tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
    "# \tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
    "# \tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n",
    "# \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n",
    "# \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n",
    "# \tat scala.Option.foreach(Option.scala:407)\n",
    "# \tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n",
    "# \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n",
    "# \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n",
    "# \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n",
    "# \tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
    "# \tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n",
    "# \tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\n",
    "# \tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2259)\n",
    "# \tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2278)\n",
    "# \tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:506)\n",
    "# \tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:459)\n",
    "# \tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\n",
    "# \tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)\n",
    "# \tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2863)\n",
    "# \tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\n",
    "# \tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n",
    "# \tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\n",
    "# \tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n",
    "# \tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n",
    "# \tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n",
    "# \tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
    "# \tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
    "# \tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\n",
    "# \tat org.apache.spark.sql.Dataset.head(Dataset.scala:2863)\n",
    "# \tat org.apache.spark.sql.Dataset.take(Dataset.scala:3084)\n",
    "# \tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)\n",
    "# \tat org.apache.spark.sql.Dataset.showString(Dataset.scala:327)\n",
    "# \tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)\n",
    "# \tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\n",
    "# \tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
    "# \tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
    "# \tat py4j.Gateway.invoke(Gateway.java:282)\n",
    "# \tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
    "# \tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
    "# \tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
    "# \tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
    "# \tat java.base/java.lang.Thread.run(Thread.java:1623)\n",
    "# Caused by: org.apache.spark.SparkException: Python worker failed to connect back.\n",
    "# \tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\n",
    "# \tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\n",
    "# \tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n",
    "# \tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:157)\n",
    "# \tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
    "# \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
    "# \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
    "# \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
    "# \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
    "# \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
    "# \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
    "# \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
    "# \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
    "# \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
    "# \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
    "# \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
    "# \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
    "# \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
    "# \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
    "# \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
    "# \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
    "# \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
    "# \tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
    "# \tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
    "# \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
    "# \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
    "# \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
    "# \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
    "# \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
    "# \t... 1 more\n",
    "# Caused by: java.net.SocketTimeoutException: Accept timed out\n",
    "# \tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\n",
    "# \tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\n",
    "# \tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\n",
    "# \tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\n",
    "# \tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\n",
    "# \tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\n",
    "# \tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\n",
    "# \tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\n",
    "# \t... 29 more\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ALS(\n",
    "#             rank=30,\n",
    "#             userCol='user_id',\n",
    "#             itemCol='item_id',\n",
    "#             ratingCol='relevance',\n",
    "#             maxIter=10,\n",
    "#             alpha=1.0,\n",
    "#             regParam=0.1,\n",
    "#             implicitPrefs=True,\n",
    "#             seed=42,\n",
    "#             coldStartStrategy='drop',\n",
    "#         ).fit(spark_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Py4JJavaError                             Traceback (most recent call last)\n",
    "# ~\\AppData\\Local\\Temp\\ipykernel_1552\\1727582100.py in <module>\n",
    "# ----> 1 model = ALS(\n",
    "#       2             rank=30,\n",
    "#       3             userCol='user_id',\n",
    "#       4             itemCol='item_id',\n",
    "#       5             ratingCol='relevance',\n",
    "\n",
    "# C:\\Conda\\lib\\site-packages\\pyspark\\ml\\base.py in fit(self, dataset, params)\n",
    "#     203                 return self.copy(params)._fit(dataset)\n",
    "#     204             else:\n",
    "# --> 205                 return self._fit(dataset)\n",
    "#     206         else:\n",
    "#     207             raise TypeError(\n",
    "\n",
    "# C:\\Conda\\lib\\site-packages\\pyspark\\ml\\wrapper.py in _fit(self, dataset)\n",
    "#     381 \n",
    "#     382     def _fit(self, dataset: DataFrame) -> JM:\n",
    "# --> 383         java_model = self._fit_java(dataset)\n",
    "#     384         model = self._create_model(java_model)\n",
    "#     385         return self._copyValues(model)\n",
    "\n",
    "# C:\\Conda\\lib\\site-packages\\pyspark\\ml\\wrapper.py in _fit_java(self, dataset)\n",
    "#     378 \n",
    "#     379         self._transfer_params_to_java()\n",
    "# --> 380         return self._java_obj.fit(dataset._jdf)\n",
    "#     381 \n",
    "#     382     def _fit(self, dataset: DataFrame) -> JM:\n",
    "\n",
    "# C:\\Conda\\lib\\site-packages\\py4j\\java_gateway.py in __call__(self, *args)\n",
    "#    1319 \n",
    "#    1320         answer = self.gateway_client.send_command(command)\n",
    "# -> 1321         return_value = get_return_value(\n",
    "#    1322             answer, self.gateway_client, self.target_id, self.name)\n",
    "#    1323 \n",
    "\n",
    "# C:\\Conda\\lib\\site-packages\\pyspark\\sql\\utils.py in deco(*a, **kw)\n",
    "#     188     def deco(*a: Any, **kw: Any) -> Any:\n",
    "#     189         try:\n",
    "# --> 190             return f(*a, **kw)\n",
    "#     191         except Py4JJavaError as e:\n",
    "#     192             converted = convert_exception(e.java_exception)\n",
    "\n",
    "# C:\\Conda\\lib\\site-packages\\py4j\\protocol.py in get_return_value(answer, gateway_client, target_id, name)\n",
    "#     324             value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n",
    "#     325             if answer[1] == REFERENCE_TYPE:\n",
    "# --> 326                 raise Py4JJavaError(\n",
    "#     327                     \"An error occurred while calling {0}{1}{2}.\\n\".\n",
    "#     328                     format(target_id, \".\", name), value)\n",
    "\n",
    "# Py4JJavaError: An error occurred while calling o62.fit.\n",
    "# : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (noutnik executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\n",
    "# \tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\n",
    "# \tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\n",
    "# \tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n",
    "# \tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:157)\n",
    "# \tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
    "# \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
    "# \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
    "# \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
    "# \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
    "# \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
    "# \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
    "# \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
    "# \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
    "# \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
    "# \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
    "# \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
    "# \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
    "# \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
    "# \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
    "# \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
    "# \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
    "# \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
    "# \tat org.apache.spark.sql.execution.SQLExecutionRDD.$anonfun$compute$1(SQLExecutionRDD.scala:52)\n",
    "# \tat org.apache.spark.sql.internal.SQLConf$.withExistingConf(SQLConf.scala:158)\n",
    "# \tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:52)\n",
    "# \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
    "# \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
    "# \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
    "# \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
    "# \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
    "# \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
    "# \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
    "# \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
    "# \tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
    "# \tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
    "# \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
    "# \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
    "# \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
    "# \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
    "# \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
    "# \tat java.base/java.lang.Thread.run(Thread.java:1623)\n",
    "# Caused by: java.net.SocketTimeoutException: Accept timed out\n",
    "# \tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\n",
    "# \tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\n",
    "# \tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\n",
    "# \tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\n",
    "# \tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\n",
    "# \tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\n",
    "# \tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\n",
    "# \tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\n",
    "# \t... 40 more\n",
    "\n",
    "# Driver stacktrace:\n",
    "# \tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n",
    "# \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n",
    "# \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n",
    "# \tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
    "# \tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
    "# \tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
    "# \tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n",
    "# \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n",
    "# \tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n",
    "# \tat scala.Option.foreach(Option.scala:407)\n",
    "# \tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n",
    "# \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n",
    "# \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n",
    "# \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n",
    "# \tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
    "# \tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n",
    "# \tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\n",
    "# \tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2259)\n",
    "# \tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2278)\n",
    "# \tat org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1470)\n",
    "# \tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
    "# \tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
    "# \tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n",
    "# \tat org.apache.spark.rdd.RDD.take(RDD.scala:1443)\n",
    "# \tat org.apache.spark.rdd.RDD.$anonfun$isEmpty$1(RDD.scala:1578)\n",
    "# \tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
    "# \tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
    "# \tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
    "# \tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n",
    "# \tat org.apache.spark.rdd.RDD.isEmpty(RDD.scala:1578)\n",
    "# \tat org.apache.spark.ml.recommendation.ALS$.train(ALS.scala:960)\n",
    "# \tat org.apache.spark.ml.recommendation.ALS.$anonfun$fit$1(ALS.scala:722)\n",
    "# \tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
    "# \tat scala.util.Try$.apply(Try.scala:213)\n",
    "# \tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
    "# \tat org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:704)\n",
    "# \tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)\n",
    "# \tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\n",
    "# \tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
    "# \tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
    "# \tat py4j.Gateway.invoke(Gateway.java:282)\n",
    "# \tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
    "# \tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
    "# \tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
    "# \tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
    "# \tat java.base/java.lang.Thread.run(Thread.java:1623)\n",
    "# Caused by: org.apache.spark.SparkException: Python worker failed to connect back.\n",
    "# \tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\n",
    "# \tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\n",
    "# \tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n",
    "# \tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:157)\n",
    "# \tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
    "# \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
    "# \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
    "# \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
    "# \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
    "# \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
    "# \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
    "# \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
    "# \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
    "# \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
    "# \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
    "# \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
    "# \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
    "# \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
    "# \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
    "# \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
    "# \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
    "# \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
    "# \tat org.apache.spark.sql.execution.SQLExecutionRDD.$anonfun$compute$1(SQLExecutionRDD.scala:52)\n",
    "# \tat org.apache.spark.sql.internal.SQLConf$.withExistingConf(SQLConf.scala:158)\n",
    "# \tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:52)\n",
    "# \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
    "# \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
    "# \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
    "# \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
    "# \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
    "# \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
    "# \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
    "# \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
    "# \tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
    "# \tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
    "# \tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
    "# \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
    "# \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
    "# \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
    "# \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
    "# \t... 1 more\n",
    "# Caused by: java.net.SocketTimeoutException: Accept timed out\n",
    "# \tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\n",
    "# \tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\n",
    "# \tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\n",
    "# \tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\n",
    "# \tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\n",
    "# \tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\n",
    "# \tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\n",
    "# \tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\n",
    "# \t... 40 more\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recs_als = model.recommendForAllUsers(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recs_als.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>manufacturer</th>\n",
       "      <th>department</th>\n",
       "      <th>brand</th>\n",
       "      <th>commodity_desc</th>\n",
       "      <th>sub_commodity_desc</th>\n",
       "      <th>curr_size_of_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25671</td>\n",
       "      <td>2</td>\n",
       "      <td>GROCERY</td>\n",
       "      <td>National</td>\n",
       "      <td>FRZN ICE</td>\n",
       "      <td>ICE - CRUSHED/CUBED</td>\n",
       "      <td>22 LB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26081</td>\n",
       "      <td>2</td>\n",
       "      <td>MISC. TRANS.</td>\n",
       "      <td>National</td>\n",
       "      <td>NO COMMODITY DESCRIPTION</td>\n",
       "      <td>NO SUBCOMMODITY DESCRIPTION</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   item_id  manufacturer    department     brand            commodity_desc  \\\n",
       "0    25671             2       GROCERY  National                  FRZN ICE   \n",
       "1    26081             2  MISC. TRANS.  National  NO COMMODITY DESCRIPTION   \n",
       "\n",
       "            sub_commodity_desc curr_size_of_product  \n",
       "0          ICE - CRUSHED/CUBED                22 LB  \n",
       "1  NO SUBCOMMODITY DESCRIPTION                       "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_features = pd.read_csv('product.csv')\n",
    "item_features.columns = [col.lower() for col in item_features.columns]\n",
    "item_features.rename(columns={'product_id': 'item_id'}, inplace=True)\n",
    "\n",
    "item_features.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_features['department'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "308"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_features['commodity_desc'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2383"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_features['sub_commodity_desc'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# берем с запасом\n",
    "recommendations = [ 26738, 26738, 26941, 25671, 26081, 26093, 18293696, 18294080, 18316298, 29247, 29252, 29340]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postfilter(recommendations, item_info, N=5):\n",
    "    \"\"\"Пост-фильтрация товаров\n",
    "    \n",
    "    Input\n",
    "    -----\n",
    "    recommendations: list\n",
    "        Ранжированный список item_id для рекомендаций\n",
    "    item_info: pd.DataFrame\n",
    "        Датафрейм с информацией о товарах\n",
    "    \"\"\"\n",
    "    \n",
    "    # Уникальность\n",
    "#     recommendations = list(set(recommendations)) - неверно! так теряется порядок\n",
    "    unique_recommendations = []\n",
    "    [unique_recommendations.append(item) for item in recommendations if item not in unique_recommendations]\n",
    "    \n",
    "    # Разные категории\n",
    "    categories_used = []\n",
    "    final_recommendations = []\n",
    "    \n",
    "    CATEGORY_NAME = 'sub_commodity_desc'\n",
    "    for item in unique_recommendations:\n",
    "        category = item_features.loc[item_features['item_id'] == item, CATEGORY_NAME].values[0]\n",
    "        \n",
    "        if category not in categories_used:\n",
    "            final_recommendations.append(item)\n",
    "            \n",
    "        unique_recommendations.remove(item)\n",
    "        categories_used.append(category)\n",
    "    \n",
    "    n_rec = len(final_recommendations)\n",
    "    if n_rec < N:\n",
    "        final_recommendations.extend(unique_recommendations[:N - n_rec])\n",
    "    else:\n",
    "        final_recommendations = final_recommendations[:N]\n",
    "    \n",
    "    assert len(final_recommendations) == N, 'Количество рекомендаций != {}'.format(N)\n",
    "    return final_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[26738, 25671, 26093, 18294080, 29247]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postfilter(recommendations, item_info=item_features, N=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
